{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import folium\n",
    "\n",
    "import credentials\n",
    "import requests\n",
    "import traceback\n",
    "\n",
    "from os import mkdir\n",
    "from os import listdir\n",
    "\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "\n",
    "#from selenium import webdriver\n",
    "#from bs4 import BeautifulSoup as bs\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from geopy import Point\n",
    "from geopy.geocoders import ArcGIS\n",
    "from geopy.distance import geodesic\n",
    "from geopy.distance import VincentyDistance\n",
    "\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_ID = credentials.credentials['CLIENT_ID'] \n",
    "CLIENT_SECRET = credentials.credentials['CLIENT_SECRET']\n",
    "ACCESS_TOKEN = credentials.credentials['ACCESS_TOKEN']\n",
    "#VERSION = '20180604'\n",
    "VERSION = '20200527'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Import and clean data from census.gov</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I Used this video to help get demographic data from census.gov :https://www.youtube.com/watch?v=K0-ifZS0mQI.\n",
    "\n",
    "Since the columns were akwardly labeled, and I was unfamiliar with the contents of the file, made sense to do a lot of the cleaning manually in MS Excel.\n",
    "\n",
    "The CSV file contains socio-economic data from 519 metro/micropolitan areas in the United States."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metro_df = pd.read_csv('DP03_All_Metros.csv')\n",
    "metro_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Make sure all Series are correct data type</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metro_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>The API returns all Null values as -999999999.0. Convert them to the label average.</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metro_df[metro_df['geographic area name'] == 'Fort Payne']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metro_df.replace(-999999999.0, np.nan, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metro_df[metro_df['geographic area name'] == 'Fort Payne']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in metro_df.columns[4:]:\n",
    "    metro_df[column].replace(np.nan, metro_df[column].mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metro_df[metro_df['geographic area name'] == 'Fort Payne']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in metro_df:\n",
    "    print((metro_df[column] == -999999999.0).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Add coordinates to metro data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "metro_df[['geographic area name', 'state']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocator = ArcGIS(user_agent = 'IBM_DS_Capstone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locateMetroCities(cities, states):\n",
    "    '''\n",
    "    cities: str, name of metro. Eg: \"Seattle-Tacoma-Bellevue\", or \"Idaho Falls\"\n",
    "    states: str, state codes. Eg: \"ID-WA\", or \"OR\"\n",
    "    '''\n",
    "    \n",
    "    city_list = cities.split('-')\n",
    "    state_list = states.split('-')\n",
    "    \n",
    "    lat_long_list = []\n",
    "    \n",
    "    for city in city_list:\n",
    "        \n",
    "        location = geolocator.geocode(city + ', ' + state_list[0])\n",
    "        \n",
    "        #makes sure we have the right city in the right state\n",
    "        if (location == None) or (location.address.split(', ')[0] != city):\n",
    "            \n",
    "            i = 1\n",
    "            while (i < len(state_list)) and (location == None):\n",
    "                \n",
    "                location = geolocator.geocode(city + ', ' + state_list[i])\n",
    "                i += 1\n",
    "        \n",
    "        lat_long_list.append([location.latitude, location.longitude])\n",
    "    \n",
    "    return lat_long_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Getting all lat-long data often times-out, collect data in batches. Remember loc is inclusive</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_long = metro_df.loc[0:74].apply(lambda x: locateMetroCities(x['geographic area name'], x['state']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_long2 = metro_df.loc[75:148].apply(lambda x: locateMetroCities(x['geographic area name'], x['state']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_long3 = metro_df.loc[149:222].apply(lambda x: locateMetroCities(x['geographic area name'], x['state']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_long4 = metro_df.loc[223:296].apply(lambda x: locateMetroCities(x['geographic area name'], x['state']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_long5 = metro_df.loc[297:370].apply(lambda x: locateMetroCities(x['geographic area name'], x['state']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_long6 = metro_df.loc[371:444].apply(lambda x: locateMetroCities(x['geographic area name'], x['state']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_long7 = metro_df.loc[445:519].apply(lambda x: locateMetroCities(x['geographic area name'], x['state']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(lat_long), len(lat_long2), len(lat_long3), len(lat_long4), len(lat_long5), len(lat_long6), len(lat_long7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Save lat_long as csv</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_long = pd.concat([lat_long, lat_long2, lat_long3, lat_long4, lat_long5, lat_long6, lat_long7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_long.to_csv('lat_long.csv', header = False, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Load lat_long from csv</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_long = pd.read_csv('lat_long.csv', header = None)\n",
    "lat_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metro_df.insert(5, 'lat_long', lat_long)\n",
    "metro_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lat_long_str_to_list_of_floats(lat_long_str):\n",
    "    \n",
    "    lat_long_str = lat_long_str.replace('[', '').replace(']', '')    \n",
    "    lat_long_list = lat_long_str.split(',')\n",
    "    \n",
    "    lat_long_pairs_count =  int(len(lat_long_list) / 2)    \n",
    "    lat_long_pairs = [[] for i in range(lat_long_pairs_count)]\n",
    "    \n",
    "    count = 0\n",
    "    for pair in lat_long_pairs:\n",
    "        \n",
    "        pair.append(float(lat_long_list[0 + count]))\n",
    "        pair.append(float(lat_long_list[1 + count]))\n",
    "        count += 2\n",
    "        \n",
    "    return lat_long_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metro_df['lat_long'] = pd.Series(map(lat_long_str_to_list_of_floats, metro_df['lat_long']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Pick a region of interest</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Pacific Northwest (WA, OR, ID). Needed the plain WA, OR, ID state codes AND the wildcards to get all the relevant metros</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PNW_metro_df = metro_df[(metro_df['state'] == 'WA') | (metro_df['state'] == 'OR') | (metro_df['state'] == 'ID') | (metro_df['state'].str.contains('WA')) == True | (metro_df['state'].str.contains('OR')) | (metro_df['state'].str.contains('ID'))]\n",
    "PNW_metro_df.reset_index(drop = True, inplace = True)\n",
    "PNW_metro_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Get restaurant type frequescies for each metro</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Get category JSON from Foursquare</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.foursquare.com/v2/venues/categories?&client_id={}&client_secret={}&v={}'.format(\n",
    "                CLIENT_ID, \n",
    "                CLIENT_SECRET,\n",
    "                VERSION)\n",
    "\n",
    "categories = requests.get(url).json()\n",
    "\n",
    "food_category_dict = categories['response']['categories'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Get all 'id' values from returned JSON</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dict_extract(key, var):\n",
    "    '''\n",
    "    https://stackoverflow.com/questions/9807634/find-all-occurrences-of-a-key-in-nested-dictionaries-and-lists\n",
    "    Title: Find all occurrences of a key in nested dictionaries and lists\n",
    "    by user: hexerei software\n",
    "    28 MAY 2020\n",
    "    \n",
    "    Pulls all values with a specific key from a nested dictionary\n",
    "    'var' is the dict\n",
    "    \n",
    "    '''\n",
    "    if hasattr(var,'items'):\n",
    "        for k, v in var.items():\n",
    "            if k == key:\n",
    "                yield v\n",
    "            if isinstance(v, dict):\n",
    "                for result in gen_dict_extract(key, v):\n",
    "                    yield result\n",
    "            elif isinstance(v, list):\n",
    "                for d in v:\n",
    "                    for result in gen_dict_extract(key, d):\n",
    "                        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = list(gen_dict_extract('id', food_category_dict))\n",
    "name_list = list(gen_dict_extract('name', food_category_dict))\n",
    "\n",
    "#remove generic 'Food' category\n",
    "id_list = id_list[1:]\n",
    "name_list = name_list[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_id_dict = {}\n",
    "id_name_dict = {}\n",
    "for i in range(0,len(name_list)):\n",
    "    name_id_dict[name_list[i]] = id_list[i]\n",
    "    id_name_dict[id_list[i]] = name_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRestaurantCoordsAndType(lat_long_list, category_list, limit = 50, radius = 25000, redo50 = False):\n",
    "\n",
    "    allVenues_df = pd.DataFrame(columns = ['name', 'category', 'category_id', 'id', 'lat', 'long'])\n",
    "    \n",
    "    for lat_long in lat_long_list:\n",
    "        \n",
    "        for category in category_list:\n",
    "\n",
    "            lat = lat_long[0]\n",
    "            long = lat_long[1]\n",
    "\n",
    "            url = 'https://api.foursquare.com/v2/venues/explore?&openNow=0&time=any&sortByPopularity=1&categoryId=' + category + '&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
    "                        CLIENT_ID, \n",
    "                        CLIENT_SECRET, \n",
    "                        VERSION, \n",
    "                        lat, \n",
    "                        long, \n",
    "                        radius, \n",
    "                        limit)\n",
    "            \n",
    "            result = requests.get(url).json()\n",
    "            \n",
    "            #Keep relevant data only\n",
    "            try:\n",
    "                #Foursquare will only return a max of 50 venues.\n",
    "                #If the max is reached, this section splits the initial radial search into several smaller searches\n",
    "                if 'groups' in result['response'] and len(result['response']['groups'][0]['items']) == 50 and redo50 == False:\n",
    "                    \n",
    "                    lat_long_expanded_list = []\n",
    "                    degree_list = [0, 45, 90, 135, 180, 225, 270, 315]                    \n",
    "                    for degree in degree_list:\n",
    "                        if degree % 90 == 0:\n",
    "                            distance = radius/2\n",
    "                        else:\n",
    "                            distance = radius * (2/5)\n",
    "                        origin = Point(lat, long)\n",
    "                        lat_long_expanded_list.append(list(geodesic(meters = distance).destination(origin, degree))[0:2])\n",
    "                    \n",
    "                    allVenues_df = pd.concat([allVenues_df, getRestaurantCoordsAndType(lat_long_expanded_list, [category], limit = 50, radius = radius/2, redo50 = True)], sort = False)\n",
    "                    \n",
    "                elif 'groups' in result['response'] and len(result['response']['groups'][0]['items']) > 0:\n",
    "                    \n",
    "                    venues_df = pd.DataFrame(list(gen_dict_extract('venue', result['response']['groups'][0])))\n",
    "                    \n",
    "                    venues_df.drop(venues_df.loc[venues_df['categories'].map(lambda x: len(x)== 0)].index, inplace = True)                    \n",
    "                    venues_df['category'] = venues_df['categories'].map(lambda x: x[0]['name'])\n",
    "                    venues_df['category_id'] = venues_df['categories'].map(lambda x:x[0]['id'])\n",
    "                    \n",
    "                    venues_df['lat'] = venues_df['location'].map(lambda x: x['lat'])\n",
    "                    venues_df['long'] = venues_df['location'].map(lambda x: x['lng'])\n",
    "                    \n",
    "                    venues_df.drop(['categories','location', 'photos'], axis = 1, inplace = True)\n",
    "\n",
    "                    if 'venuePage' in venues_df:\n",
    "                        venues_df.drop(['venuePage'], axis = 1, inplace = True)\n",
    "                    if 'delivery' in venues_df:\n",
    "                        venues_df.drop(['delivery'], axis = 1, inplace = True)\n",
    "                    if 'events' in venues_df:\n",
    "                        venues_df.drop(['events'], axis = 1, inplace = True)\n",
    "                    \n",
    "                    allVenues_df = pd.concat([allVenues_df, venues_df], sort = False)\n",
    "                    \n",
    "            except KeyError as err:\n",
    "\n",
    "                print('KeyError for cat: ' + category + ' at ' + str(lat) + ', ' + str(long))\n",
    "                traceback.print_exc()\n",
    "                \n",
    "                continue\n",
    "                \n",
    "            except IndexError as err:\n",
    "                \n",
    "                print('IndexError for cat: ' + category + ' at ' + str(lat) + ', ' + str(long))\n",
    "                traceback.print_exc()\n",
    "            \n",
    "            except TypeError as err:\n",
    "                \n",
    "                print('TypeError for cat: ' + category + ' at ' + str(lat) + ', ' + str(long))\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    allVenues_df.drop_duplicates(subset = 'id', keep = 'last', inplace = True)\n",
    "    \n",
    "    allVenues_df = allVenues_df[allVenues_df['category'] != 'Gas Station']\n",
    "    allVenues_df = allVenues_df[allVenues_df['category'] != 'Grocery Store']\n",
    "    allVenues_df = allVenues_df[allVenues_df['category'] != 'Food Court']\n",
    "    allVenues_df = allVenues_df[allVenues_df['category'] != 'Food']\n",
    "    #allVenues_df.drop(allVenues_df.loc[allVenues_df['category'] == 'Fast Food Restaurant'].index, inplace = True)\n",
    "    allVenues_df = allVenues_df[allVenues_df['category'] != 'Fast Food Restaurant']\n",
    "    allVenues_df = allVenues_df[allVenues_df['category'] != 'Coffee Shop']\n",
    "    allVenues_df = allVenues_df[allVenues_df['category'] != 'Shopping Plaza']\n",
    "    \n",
    "    if redo50 == False:\n",
    "        allVenues_df.rename(columns = {'id' : 'venue_id'}, inplace = True)\n",
    "    allVenues_df.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    return allVenues_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Foursquare only allows 5000 API calls an hour, so batch the calls and delay between batches</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metro_venue_dict = {}\n",
    "metro_venue_dict2 = {}\n",
    "metro_venue_dict3 = {}\n",
    "metro_venue_dict4 = {}\n",
    "metro_venue_dict5 = {}\n",
    "\n",
    "print('Starting batch 1 at ' + datetime.now().strftime(\"%H:%M:%S\") + '\\n')\n",
    "for i in PNW_metro_df.loc[0 : 4].index:    \n",
    "    metro_venue_dict[PNW_metro_df['geographic area name'][i]] = getRestaurantCoordsAndType(PNW_metro_df['lat_long'][i], id_list)\n",
    "\n",
    "print('Batch complete. 45min cooldown started at ' + datetime.now().strftime(\"%H:%M:%S\") + '\\n\\n')\n",
    "sleep(60*45)\n",
    "\n",
    "print('Starting batch 2 at ' + datetime.now().strftime(\"%H:%M:%S\") + '\\n')\n",
    "for i in PNW_metro_df.loc[5 : 9].index:    \n",
    "    metro_venue_dict2[PNW_metro_df['geographic area name'][i]] = getRestaurantCoordsAndType(PNW_metro_df['lat_long'][i], id_list)\n",
    "\n",
    "print('Batch complete. 45min cooldown started at ' + datetime.now().strftime(\"%H:%M:%S\") + '\\n\\n')\n",
    "sleep(60*45)\n",
    "\n",
    "print('Starting batch 3 at ' + datetime.now().strftime(\"%H:%M:%S\") + '\\n')\n",
    "for i in PNW_metro_df.loc[10 : 14].index:    \n",
    "    metro_venue_dict3[PNW_metro_df['geographic area name'][i]] = getRestaurantCoordsAndType(PNW_metro_df['lat_long'][i], id_list)\n",
    "\n",
    "print('Batch complete. 45min cooldown started at ' + datetime.now().strftime(\"%H:%M:%S\") + '\\n\\n')\n",
    "sleep(60*45)\n",
    "\n",
    "print('Starting batch 4 at ' + datetime.now().strftime(\"%H:%M:%S\") + '\\n')\n",
    "for i in PNW_metro_df.loc[15 : 19].index:    \n",
    "    metro_venue_dict4[PNW_metro_df['geographic area name'][i]] = getRestaurantCoordsAndType(PNW_metro_df['lat_long'][i], id_list)\n",
    "\n",
    "print('Batch complete. 45min cooldown started at ' + datetime.now().strftime(\"%H:%M:%S\") + '\\n\\n')\n",
    "sleep(60*45)\n",
    "\n",
    "print('Starting batch 5 at ' + datetime.now().strftime(\"%H:%M:%S\") + '\\n')\n",
    "for i in PNW_metro_df.loc[20 : 24].index:    \n",
    "    metro_venue_dict5[PNW_metro_df['geographic area name'][i]] = getRestaurantCoordsAndType(PNW_metro_df['lat_long'][i], id_list)\n",
    "\n",
    "print('Batch complete. 45min cooldown started at ' + datetime.now().strftime(\"%H:%M:%S\") + '\\n\\n')\n",
    "sleep(60*45)\n",
    "\n",
    "print('Starting batch 6 at ' + datetime.now().strftime(\"%H:%M:%S\") + '\\n')\n",
    "for i in PNW_metro_df.loc[25 : 29].index:    \n",
    "    metro_venue_dict6[PNW_metro_df['geographic area name'][i]] = getRestaurantCoordsAndType(PNW_metro_df['lat_long'][i], id_list)\n",
    "\n",
    "print('Batch complete. 45min cooldown started at ' + datetime.now().strftime(\"%H:%M:%S\") + '\\n\\n')\n",
    "sleep(60*45)\n",
    "\n",
    "print('Starting batch 7 at ' + datetime.now().strftime(\"%H:%M:%S\") + '\\n')\n",
    "for i in PNW_metro_df.loc[30 : 34].index:    \n",
    "    metro_venue_dict7[PNW_metro_df['geographic area name'][i]] = getRestaurantCoordsAndType(PNW_metro_df['lat_long'][i], id_list)\n",
    "\n",
    "    \n",
    "print('Final batch complete at ' + datetime.now().strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Combine all dicts</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metro_venue_dict = {**metro_venue_dict, **metro_venue_dict2, **metro_venue_dict3, **metro_venue_dict4, **metro_venue_dict5, **metro_venue_dict6, **metro_venue_dict7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mkdir('./metro_venues')\n",
    "    \n",
    "for key in metro_venue_dict.keys():\n",
    "    metro_venue_dict[key].to_csv('./metro_venues/' + key + '.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Recreate dict from csv files</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metro_venue_dict = {}\n",
    "\n",
    "for file in listdir('./metro_venues'):\n",
    "    metro_venue_dict[file[0:-4]] = pd.read_csv('./metro_venues/' + file)\n",
    "\n",
    "metro_venue_dict['Aberdeen'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cluster Metros</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Calculate frequency of restaurant types in each metro</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venue_frequency_df = pd.DataFrame(name_list)\n",
    "venue_frequency_df.set_index(0, inplace = True)\n",
    "del venue_frequency_df.index.name\n",
    "\n",
    "for key in metro_venue_dict.keys():\n",
    "    \n",
    "    venue_frequency_df[key] = metro_venue_dict[key]['category'].value_counts()\n",
    "\n",
    "venue_frequency_df.replace(np.nan, 0, inplace = True)\n",
    "\n",
    "venue_frequency_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Normalise frequencies</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "frequency_normed_df = pd.DataFrame(min_max_scaler.fit_transform(venue_frequency_df))\n",
    "\n",
    "frequency_normed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Fit K-means model</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "\n",
    "kmeans = KMeans(n_clusters = k).fit(frequency_normed_df.transpose())\n",
    "kmeans.labels_[0:5]#sneakpeek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Insert cluster labels into metro_df</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    PNW_metro_df.insert(0, 'cluster', kmeans.labels_)\n",
    "except ValueError:\n",
    "    PNW_metro_df['cluster'] = kmeans.labels_\n",
    "    \n",
    "PNW_metro_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PNW_metro_df['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Visualise the metro clusters</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Colour-code and overlay the metros on a map</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_clusters = folium.Map(width = '100%', height = '100%', location = [45.3, -118], zoom_start = 6, tiles = \"Stamen Toner\")\n",
    "\n",
    "x = np.arange(k)\n",
    "ys = [i + x + (i*x)**2 for i in range(k)]\n",
    "colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\n",
    "rainbow = [colors.rgb2hex(i) for i in colors_array]\n",
    "\n",
    "# add markers to the map\n",
    "markers_colors = []\n",
    "\n",
    "for i in PNW_metro_df.index:\n",
    "    lat = PNW_metro_df['lat_long'][i][0][0]\n",
    "    long = PNW_metro_df['lat_long'][i][0][1]\n",
    "    metro = PNW_metro_df['geographic area name'][i]\n",
    "    pop = PNW_metro_df['population_16_up'][i]\n",
    "    cluster = PNW_metro_df['cluster'][i]\n",
    "    \n",
    "    label = folium.Popup(str(metro) + '\\nPopulation: ' + str(pop) + '\\nCluster: ' + str(cluster), parse_html=True)\n",
    "    \n",
    "    folium.CircleMarker(\n",
    "        [lat, long],\n",
    "        radius=3,\n",
    "        popup=label,\n",
    "        color=rainbow[cluster],\n",
    "        fill=True,\n",
    "        fill_color=rainbow[cluster],\n",
    "        fill_opacity=0.7).add_to(map_clusters)\n",
    "       \n",
    "map_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The map above shows the various metro/micropolitan areas in Washington, Oregon, and Idaho. The areas have been K-means clustered according to the ratio of different restaurant types in each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Calculate mean and std of each cluster<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_normed_df.columns = venue_frequency_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_normed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Calculate mean and standard dev of each cluster. Create DF for each cluster<h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_mean_std = [[[],[]], [[],[]], [[],[]]]\n",
    "\n",
    "cluster_mean_std[0][0] = frequency_normed_df[PNW_metro_df[PNW_metro_df['cluster'] == 0]['geographic area name']].mean(axis = 1)\n",
    "cluster_mean_std[0][1] = frequency_normed_df[PNW_metro_df[PNW_metro_df['cluster'] == 0]['geographic area name']].std(axis = 1)\n",
    "\n",
    "cluster_mean_std[1][0] = frequency_normed_df[PNW_metro_df[PNW_metro_df['cluster'] == 1]['geographic area name']].mean(axis = 1)\n",
    "cluster_mean_std[1][1] = frequency_normed_df[PNW_metro_df[PNW_metro_df['cluster'] == 1]['geographic area name']].std(axis = 1)\n",
    "\n",
    "cluster_mean_std[2][0] = frequency_normed_df[PNW_metro_df[PNW_metro_df['cluster'] == 2]['geographic area name']].mean(axis = 1)\n",
    "cluster_mean_std[2][1] = frequency_normed_df[PNW_metro_df[PNW_metro_df['cluster'] == 2]['geographic area name']].std(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster0_mean = frequency_normed_df[PNW_metro_df[PNW_metro_df['cluster'] == 0]['geographic area name']].mean(axis = 1)\n",
    "cluster1_mean = frequency_normed_df[PNW_metro_df[PNW_metro_df['cluster'] == 1]['geographic area name']].mean(axis = 1)\n",
    "cluster2_mean = frequency_normed_df[PNW_metro_df[PNW_metro_df['cluster'] == 2]['geographic area name']].mean(axis = 1)\n",
    "\n",
    "cluster0_std = frequency_normed_df[PNW_metro_df[PNW_metro_df['cluster'] == 0]['geographic area name']].std(axis = 1)\n",
    "cluster1_std = frequency_normed_df[PNW_metro_df[PNW_metro_df['cluster'] == 1]['geographic area name']].std(axis = 1)\n",
    "cluster2_std = frequency_normed_df[PNW_metro_df[PNW_metro_df['cluster'] == 2]['geographic area name']].std(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster0_df = frequency_normed_df[PNW_metro_df[PNW_metro_df['cluster'] == 0]['geographic area name']]\n",
    "cluster1_df = frequency_normed_df[PNW_metro_df[PNW_metro_df['cluster'] == 1]['geographic area name']]\n",
    "cluster2_df = frequency_normed_df[PNW_metro_df[PNW_metro_df['cluster'] == 2]['geographic area name']]\n",
    "\n",
    "cluster0_df.insert(0,'cluster_std', cluster0_std)\n",
    "cluster0_df.insert(0,'cluster_mean', cluster0_mean)\n",
    "cluster1_df.insert(0,'cluster_std', cluster1_std)\n",
    "cluster1_df.insert(0,'cluster_mean', cluster1_mean)\n",
    "cluster2_df.insert(0,'cluster_std', cluster2_std)\n",
    "cluster2_df.insert(0,'cluster_mean', cluster2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster0_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Index cluster DFs with restaurant types</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster0_df.index = venue_frequency_df.index\n",
    "cluster1_df.index = venue_frequency_df.index\n",
    "cluster2_df.index = venue_frequency_df.index\n",
    "\n",
    "cluster0_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Observe what restaurant types are most under represented in what cities</h3>\n",
    "    <h5>Calculate how many stds each restaurant type frequency is away from the mean in each city</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_list = [[],[],[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subtract mean from frequecy\n",
    "std0_df = cluster0_df[cluster0_df.columns[2:23]].subtract(cluster0_df['cluster_mean'], axis = 0)\n",
    "std1_df = cluster1_df[cluster1_df.columns[2:23]].subtract(cluster1_df['cluster_mean'], axis = 0)\n",
    "std2_df = cluster2_df[cluster2_df.columns[2:23]].subtract(cluster2_df['cluster_mean'], axis = 0)\n",
    "\n",
    "#divide by std\n",
    "std0_df = std0_df[std0_df.columns[:]].div(cluster0_df['cluster_std'], axis = 0)\n",
    "std1_df = std1_df[std1_df.columns[:]].div(cluster1_df['cluster_std'], axis = 0)\n",
    "std2_df = std2_df[std2_df.columns[:]].div(cluster2_df['cluster_std'], axis = 0)\n",
    "\n",
    "std_list[0] = std0_df\n",
    "std_list[1] = std1_df\n",
    "std_list[2] = std2_df\n",
    "\n",
    "#zero out NaNs\n",
    "# std0_df.replace(np.nan, 0, inplace = True)\n",
    "# std1_df.replace(np.nan, 0, inplace = True)\n",
    "# std2_df.replace(np.nan, 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std0_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_list[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Make reccomendations on what cities would be best for what restaurants</h3>\n",
    "<h5>Calculate population/restaurant</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_venue_ratio_list = []\n",
    "\n",
    "for metro in PNW_metro_df['geographic area name']:\n",
    "    \n",
    "    pop_venue_ratio_list.append(float(PNW_metro_df[PNW_metro_df['geographic area name'] == metro]['population_16_up']) / len(metro_venue_dict[metro]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PNW_metro_df.insert(5, 'pop_venue_ratio', pop_venue_ratio_list)\n",
    "\n",
    "PNW_metro_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Find top 3 metros with highest population/venue ratios</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_pop_venue_ratio_df = PNW_metro_df.nlargest(3, [\"pop_venue_ratio\"], keep = 'all')[['cluster', 'geographic area name', 'pop_venue_ratio']]\n",
    "top_pop_venue_ratio_df.reset_index(inplace = True, drop = True)\n",
    "top_pop_venue_ratio_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Find 3 least represented restaurant types in each above city</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(std_list[top_pop_venue_ratio_df.iloc[0]['cluster']].nsmallest(3, [top_pop_venue_ratio_df.iloc[0]['geographic area name']], keep = 'all')[top_pop_venue_ratio_df.iloc[0]['geographic area name']], '\\n')\n",
    "print(std_list[top_pop_venue_ratio_df.iloc[1]['cluster']].nsmallest(3, [top_pop_venue_ratio_df.iloc[1]['geographic area name']], keep = 'all')[top_pop_venue_ratio_df.iloc[1]['geographic area name']], '\\n')\n",
    "print(std_list[top_pop_venue_ratio_df.iloc[2]['cluster']].nsmallest(3, [top_pop_venue_ratio_df.iloc[2]['geographic area name']], keep = 'all')[top_pop_venue_ratio_df.iloc[2]['geographic area name']], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Get most underreprented restaurant type in each of the three above cities</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_underrepresented = [[], [], []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_underrepresented[0] = std_list[top_pop_venue_ratio_df.iloc[0]['cluster']].nsmallest(3, [top_pop_venue_ratio_df.iloc[0]['geographic area name']], keep = 'all')[top_pop_venue_ratio_df.iloc[0]['geographic area name']].index.values\n",
    "top_underrepresented[1] = std_list[top_pop_venue_ratio_df.iloc[1]['cluster']].nsmallest(3, [top_pop_venue_ratio_df.iloc[1]['geographic area name']], keep = 'all')[top_pop_venue_ratio_df.iloc[1]['geographic area name']].index.values\n",
    "top_underrepresented[2] = std_list[top_pop_venue_ratio_df.iloc[2]['cluster']].nsmallest(3, [top_pop_venue_ratio_df.iloc[2]['geographic area name']], keep = 'all')[top_pop_venue_ratio_df.iloc[2]['geographic area name']].index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Calculate the best locations for the most underrepresented restaurants, within their respective metros</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Give our selected metros an \"intrametro cluster\" column</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metro_venue_dict[top_pop_venue_ratio_df.iloc[0]['geographic area name']]['intrametro cluster'] = 'not yet'\n",
    "metro_venue_dict[top_pop_venue_ratio_df.iloc[1]['geographic area name']]['intrametro cluster'] = 'not yet'\n",
    "metro_venue_dict[top_pop_venue_ratio_df.iloc[2]['geographic area name']]['intrametro cluster'] = 'not yet'\n",
    "\n",
    "metro_venue_dict[top_pop_venue_ratio_df.iloc[0]['geographic area name']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metro_venue_dict[top_pop_venue_ratio_df.iloc[0]['geographic area name']]['intrametro cluster'].value_counts()['not yet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Cluster Restaurants in our three metros<br/>Use DBSCAN because of non-random distribution of buildings along streets</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resto_lat_long_list = []\n",
    "resto_lat_long_list.append(metro_venue_dict[top_pop_venue_ratio_df.iloc[0]['geographic area name']][['lat', 'long']].to_numpy())\n",
    "resto_lat_long_list.append(metro_venue_dict[top_pop_venue_ratio_df.iloc[1]['geographic area name']][['lat', 'long']].to_numpy())\n",
    "resto_lat_long_list.append(metro_venue_dict[top_pop_venue_ratio_df.iloc[2]['geographic area name']][['lat', 'long']].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.004\n",
    "dbsacn_list = []\n",
    "dbsacn_list.append(DBSCAN(eps = eps, min_samples = 5).fit(resto_lat_long_list[0]))\n",
    "dbsacn_list.append(DBSCAN(eps = eps, min_samples = 5).fit(resto_lat_long_list[1]))\n",
    "dbsacn_list.append(DBSCAN(eps = eps, min_samples = 5).fit(resto_lat_long_list[2]))\n",
    "\n",
    "dbsacn_list[0].labels_[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metro_venue_dict[top_pop_venue_ratio_df.iloc[0]['geographic area name']]['intrametro cluster'] = dbsacn_list[0].labels_\n",
    "metro_venue_dict[top_pop_venue_ratio_df.iloc[1]['geographic area name']]['intrametro cluster'] = dbsacn_list[1].labels_\n",
    "metro_venue_dict[top_pop_venue_ratio_df.iloc[2]['geographic area name']]['intrametro cluster'] = dbsacn_list[2].labels_\n",
    "\n",
    "metro_venue_dict[top_pop_venue_ratio_df.iloc[0]['geographic area name']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Visualise top 10 most populated clusters</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMostPopulatedClusters(metro, topXclusters = 10, viewOutliers = False):\n",
    "    \n",
    "    cluster_value_counts = pd.DataFrame(metro_venue_dict[metro]['intrametro cluster'].value_counts())\n",
    "    clusters = list(cluster_value_counts[0:topXclusters].index)\n",
    "\n",
    "    #Remove -1 (outliers) from clusters\n",
    "    if -1 in clusters and viewOutliers == False:   \n",
    "        outlier_index = clusters.index(-1)\n",
    "        \n",
    "        if len(cluster_value_counts) > topXclusters:\n",
    "            clusters[outlier_index] = cluster_value_counts.index[topXclusters]\n",
    "        else:\n",
    "            clusters.pop(outlier_index)\n",
    "        \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_list = []\n",
    "clusters_list.append(getMostPopulatedClusters(top_pop_venue_ratio_df.iloc[0]['geographic area name']))\n",
    "clusters_list.append(getMostPopulatedClusters(top_pop_venue_ratio_df.iloc[1]['geographic area name']))\n",
    "clusters_list.append(getMostPopulatedClusters(top_pop_venue_ratio_df.iloc[2]['geographic area name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_list = []\n",
    "k_list.append(len(clusters_list[0]))\n",
    "k_list.append(len(clusters_list[1]))\n",
    "k_list.append(len(clusters_list[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualiseVenueClusters(metro, k, clusters, mapType = \"Stamen Toner\"):\n",
    "    \n",
    "    map_ = folium.Map(width = '100%',\n",
    "                      height = '100%',\n",
    "                      location =  metro_df[metro_df['geographic area name'] == metro]['lat_long'].values[0][0],\n",
    "                      zoom_start = 9,\n",
    "                      tiles = mapType)\n",
    "\n",
    "    x = np.arange(k)\n",
    "    ys = [i + x + (i*x)**2 for i in range(k)]\n",
    "    colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\n",
    "    rainbow = [colors.rgb2hex(i) for i in colors_array]\n",
    "\n",
    "    # add markers to the map\n",
    "    markers_colors = []\n",
    "\n",
    "    for i in metro_venue_dict[metro].index:\n",
    "\n",
    "        cluster = metro_venue_dict[metro]['intrametro cluster'][i]\n",
    "        if cluster in clusters:\n",
    "\n",
    "            lat = metro_venue_dict[metro]['lat'][i]\n",
    "            long = metro_venue_dict[metro]['long'][i]\n",
    "            venue = metro_venue_dict[metro]['name'][i]\n",
    "            category = metro_venue_dict[metro]['category'][i]\n",
    "            cluster_index = clusters.index(metro_venue_dict[metro]['intrametro cluster'][i])\n",
    "\n",
    "            #label = folium.Popup(str(metro) + '\\nCluster: ' + str(cluster), parse_html=True)\n",
    "            label = folium.Popup(str(venue) + ': ' + str(category) +'\\n\\nCluster: ' + str(cluster), parse_html=True)\n",
    "\n",
    "            folium.CircleMarker(\n",
    "                [lat, long],\n",
    "                radius=3,\n",
    "                popup=label,\n",
    "                color=rainbow[cluster_index],\n",
    "                fill=True,\n",
    "                fill_color=rainbow[cluster_index],\n",
    "                fill_opacity=0.5).add_to(map_)\n",
    "\n",
    "    return map_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualiseVenueClusters(top_pop_venue_ratio_df.iloc[0]['geographic area name'], k_list[0], clusters_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualiseVenueClusters(top_pop_venue_ratio_df.iloc[1]['geographic area name'], k_list[1], clusters_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualiseVenueClusters(top_pop_venue_ratio_df.iloc[2]['geographic area name'], k_list[2], clusters_list[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Find centroids of clusters</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findCentroid(an_array):\n",
    "   \n",
    "    count = an_array.shape[0]\n",
    "    \n",
    "    sum_lat = np.sum(an_array[:, 0])\n",
    "    sum_long = np.sum(an_array[:, 1])\n",
    "    \n",
    "    lat_centroid = sum_lat/count\n",
    "    long_centroid = sum_long/count\n",
    "    \n",
    "    return lat_centroid, long_centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findClusterCentroid(metro_venue_df, metro_clusters):\n",
    "\n",
    "    venuesInTopClusters_df = metro_venue_df[metro_venue_df['intrametro cluster'].isin(metro_clusters)]\n",
    "\n",
    "    centroid_list = []\n",
    "    for cluster in metro_clusters:\n",
    "\n",
    "        cluster_lat_long_array = venuesInTopClusters_df[venuesInTopClusters_df['intrametro cluster'] == cluster][['lat', 'long']].to_numpy()\n",
    "        centroid_list.append([findCentroid(cluster_lat_long_array)])\n",
    "\n",
    "    return centroid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_list = []\n",
    "centroid_list.append(findClusterCentroid(metro_venue_dict[top_pop_venue_ratio_df.iloc[0]['geographic area name']], clusters_list[0]))\n",
    "centroid_list.append(findClusterCentroid(metro_venue_dict[top_pop_venue_ratio_df.iloc[1]['geographic area name']], clusters_list[1]))\n",
    "centroid_list.append(findClusterCentroid(metro_venue_dict[top_pop_venue_ratio_df.iloc[2]['geographic area name']], clusters_list[2]))\n",
    "\n",
    "centroid_list[0][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroidDF_list = []\n",
    "\n",
    "for i in range(k):    \n",
    "    centroid_df = pd.DataFrame(clusters_list[i], pd.Series(centroid_list[i])).reset_index()\n",
    "    centroid_df.rename(columns = {'index' : 'lat_long', 0 : 'cluster'}, inplace = True)\n",
    "    centroidDF_list.append(centroid_df)\n",
    "\n",
    "# seattle_centroid_df = pd.DataFrame(clusters_seattle, pd.Series(seattle_centroid_list)).reset_index()\n",
    "# seattle_centroid_df.rename(columns = {'index' : 'lat_long', 0 : 'cluster'}, inplace = True)\n",
    "\n",
    "# moses_lake_centroid_df = pd.DataFrame(clusters_moses_lake, pd.Series(moses_lake_centroid_list)).reset_index()\n",
    "# moses_lake_centroid_df.rename(columns = {'index' : 'lat_long', 0 : 'cluster'}, inplace = True)\n",
    "\n",
    "centroidDF_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Find clusters that don't contain the underrepresented restaurant type<br/>Find clusters that are furtherest away from that restaurant type<h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# venues_incluster_list = []\n",
    "# clusters_with_top_list = []\n",
    "# clusters_wout_top_list = []\n",
    "# min_distance_list = []\n",
    "\n",
    "# top_venues_list = []\n",
    "# min_distance_df_list =[]\n",
    "\n",
    "# for i in range(k):\n",
    "\n",
    "#     venues_incluster_list.append(metro_venue_dict[top_pop_venue_ratio_df.iloc[i]['geographic area name']][(metro_venue_dict[top_pop_venue_ratio_df.iloc[i]['geographic area name']]['intrametro cluster'].isin(clusters_list[i]))])\n",
    "#     clusters_with_top_list.append(venues_incluster_list[i][venues_incluster_list[i]['category'] == top_underrepresented[i][0]]['intrametro cluster'])\n",
    "#     clusters_wout_top_list.append(pd.Series(clusters_list[i])[pd.Series(clusters_list[i]).isin(clusters_with_top_list[i]) == False].values)\n",
    "\n",
    "#     min_distance_list.append([])\n",
    "#     top_venues_list.append(venues_incluster_list[i][venues_incluster_list[i]['category'] == top_underrepresented[i][0]])\n",
    "#     if len(top_venues_list) > 0:\n",
    "#         for cluster in clusters_wout_top_list[i]:\n",
    "\n",
    "#             (lat_0, long_0) = centroidDF_list[i][centroidDF_list[i]['cluster'] == cluster]['lat_long'].values[0][0]\n",
    "#             distance_min = (top_venues_list[i].apply(lambda x: geodesic((lat_0, long_0), (x['lat'], x['long'])).meters, axis = 1).min())\n",
    "#             print(distance_min)#***TEST***TEST***\n",
    "#             min_distance_list[i].append([cluster, distance_min])\n",
    "#             min_distance_df_list.append(pd.DataFrame(min_distance_list[i]).set_index(0).rename(columns = {1 : 'distance'}))\n",
    "    \n",
    "\n",
    "venues_incluster_0 = metro_venue_dict[top_pop_venue_ratio_df.iloc[0]['geographic area name']][(metro_venue_dict[top_pop_venue_ratio_df.iloc[0]['geographic area name']]['intrametro cluster'].isin(clusters_list[0]))]\n",
    "clusters_with_top_0 = venues_incluster_0[venues_incluster_0['category'] == top_underrepresented[0][0]]['intrametro cluster']\n",
    "clusters_wout_top_0 = pd.Series(clusters_list[0])[pd.Series(clusters_list[0]).isin(clusters_with_top_0) == False].values\n",
    "\n",
    "min_distance_list_0 = []\n",
    "if len(clusters_wout_top_0) > 0:\n",
    "    recommendedClusters = clusters_wout_top_0\n",
    "else:\n",
    "    recommendedClusters = clusters_with_top_0\n",
    "    \n",
    "for cluster in recommendedClusters:\n",
    "    \n",
    "    (lat_0, long_0) = centroidDF_list[0][centroidDF_list[0]['cluster'] == cluster]['lat_long'].values[0][0]    \n",
    "    top_venues_0 = venues_incluster_0[venues_incluster_0['category'] == top_underrepresented[0][0]]\n",
    "    distance_min = top_venues_0.apply(lambda x: geodesic((lat_0, long_0), (x['lat'], x['long'])).meters, axis = 1).min()\n",
    "    if str(type(distance_min)) == \"<class 'pandas.core.series.Series'>\":\n",
    "        distance_min = -1\n",
    "    min_distance_list_0.append([cluster, distance_min])\n",
    "    min_distance_df_0 = pd.DataFrame(min_distance_list_0).set_index(0).rename(columns = {1 : 'distance'})\n",
    "    \n",
    "venues_incluster_1 = metro_venue_dict[top_pop_venue_ratio_df.iloc[1]['geographic area name']][(metro_venue_dict[top_pop_venue_ratio_df.iloc[1]['geographic area name']]['intrametro cluster'].isin(clusters_list[1]))]\n",
    "clusters_with_top_1 = venues_incluster_1[venues_incluster_1['category'] == top_underrepresented[1][0]]['intrametro cluster']\n",
    "clusters_wout_top_1 = pd.Series(clusters_list[1])[pd.Series(clusters_list[1]).isin(clusters_with_top_1) == False].values\n",
    "\n",
    "min_distance_list_1 = []\n",
    "if len(clusters_wout_top_1) > 0:\n",
    "    recommendedClusters = clusters_wout_top_1\n",
    "else:\n",
    "    recommendedClusters = clusters_with_top_1\n",
    "    \n",
    "for cluster in recommendedClusters:\n",
    "    \n",
    "    (lat_0, long_0) = centroidDF_list[1][centroidDF_list[1]['cluster'] == cluster]['lat_long'].values[0][0]    \n",
    "    top_venues_1 = venues_incluster_1[venues_incluster_1['category'] == top_underrepresented[1][0]]\n",
    "    distance_min = top_venues_1.apply(lambda x: geodesic((lat_0, long_0), (x['lat'], x['long'])).meters, axis = 1).min()    \n",
    "    if str(type(distance_min)) == \"<class 'pandas.core.series.Series'>\":\n",
    "        distance_min = -1    \n",
    "    min_distance_list_1.append([cluster, distance_min])\n",
    "    min_distance_df_1 = pd.DataFrame(min_distance_list_1).set_index(0).rename(columns = {1 : 'distance'})\n",
    "    \n",
    "venues_incluster_2 = metro_venue_dict[top_pop_venue_ratio_df.iloc[2]['geographic area name']][(metro_venue_dict[top_pop_venue_ratio_df.iloc[2]['geographic area name']]['intrametro cluster'].isin(clusters_list[2]))]\n",
    "clusters_with_top_2 = venues_incluster_2[venues_incluster_2['category'] == top_underrepresented[2][1]]['intrametro cluster']\n",
    "clusters_wout_top_2 = pd.Series(clusters_list[2])[pd.Series(clusters_list[2]).isin(clusters_with_top_2) == False].values\n",
    "\n",
    "min_distance_list_2 = []\n",
    "if len(clusters_wout_top_2) > 0:\n",
    "    recommendedClusters = clusters_wout_top_2\n",
    "else:\n",
    "    recommendedClusters = clusters_with_top_2\n",
    "    \n",
    "for cluster in recommendedClusters:\n",
    "    \n",
    "    (lat_0, long_0) = centroidDF_list[2][centroidDF_list[2]['cluster'] == cluster]['lat_long'].values[0][0]    \n",
    "    top_venues_2 = venues_incluster_2[venues_incluster_2['category'] == top_underrepresented[2][1]]\n",
    "    distance_min = top_venues_2.apply(lambda x: geodesic((lat_0, long_0), (x['lat'], x['long'])).meters, axis = 1).min()\n",
    "    if str(type(distance_min)) == \"<class 'pandas.core.series.Series'>\":\n",
    "        distance_min = -1\n",
    "    min_distance_list_2.append([cluster, distance_min])\n",
    "    min_distance_df_2 = pd.DataFrame(min_distance_list_2).set_index(0).rename(columns = {1 : 'distance'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualiseRecommendation(metro, topVenues_df, centroid_df, minDistance_df, category, mapType = \"Stamen Toner\"):\n",
    "    \n",
    "    map_ = folium.Map(width = '100%',\n",
    "                      height = '100%',\n",
    "                      location =  metro_df[metro_df['geographic area name'] == metro]['lat_long'].values[0][0],\n",
    "                      zoom_start = 9,\n",
    "                      tiles = mapType)\n",
    "\n",
    "    for i in topVenues_df.index:\n",
    "\n",
    "        lat = metro_venue_dict[metro]['lat'][i]\n",
    "        long = metro_venue_dict[metro]['long'][i]\n",
    "        venue = metro_venue_dict[metro]['name'][i]\n",
    "\n",
    "        #label = folium.Popup(str(metro) + '\\nCluster: ' + str(cluster), parse_html=True)\n",
    "        label = folium.Popup(str(venue) + ': ' + str(category), parse_html=True)\n",
    "\n",
    "        folium.CircleMarker(\n",
    "            [lat, long],\n",
    "            radius = 5,\n",
    "            popup = label,\n",
    "            color = '#FF0000',\n",
    "            fill = True,\n",
    "            fill_color = '#FF0000',\n",
    "            fill_opacity = 1).add_to(map_)\n",
    "        \n",
    "    for i in minDistance_df.index:\n",
    "        \n",
    "        lat = centroid_df[centroid_df['cluster'] == i]['lat_long'].values[0][0][0]\n",
    "        long = centroid_df[centroid_df['cluster'] == i]['lat_long'].values[0][0][1]\n",
    "        distance = minDistance_df.loc[i]['distance']\n",
    "        \n",
    "        if distance == -1:\n",
    "            label = folium.Popup('Open ' + str(category) + ' here: ' + str(round(lat, 4)) + ', ' + str(round(long, 4)) + '<br>No ' + str(category) + ' in metro.', max_width = 400)\n",
    "        else:\n",
    "            label = folium.Popup('Open ' + str(category) + ' here: ' + str(round(lat, 4)) + ', ' + str(round(long, 4)) + '<br>Nearest ' + str(category) + ' is ' + str(round(distance / 1000, 3)) + 'km away.', max_width = 400)\n",
    "        \n",
    "        folium.Circle(\n",
    "            [lat, long],\n",
    "            radius = 500,\n",
    "            popup = label,\n",
    "            color = '#6EB5FF',\n",
    "            fill = True,\n",
    "            fill_color = '#6EB5FF',\n",
    "            fill_opacity = 0.3).add_to(map_)\n",
    "        \n",
    "    return map_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualiseRecommendation(top_pop_venue_ratio_df.iloc[0]['geographic area name'], top_venues_0, centroidDF_list[0], min_distance_df_0, top_underrepresented[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualiseRecommendation(top_pop_venue_ratio_df.iloc[1]['geographic area name'], top_venues_1, centroidDF_list[1], min_distance_df_1, top_underrepresented[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualiseRecommendation(top_pop_venue_ratio_df.iloc[2]['geographic area name'], top_venues_2, centroidDF_list[2], min_distance_df_2, top_underrepresented[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
